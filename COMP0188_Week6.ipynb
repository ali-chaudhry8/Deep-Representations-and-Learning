{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4MlR3m-_5CZ",
        "tags": [
          "pdf-ignore"
        ]
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "This notebook has 4 parts. \n",
        "\n",
        "1. Part I, Preparation: load the CIFAR-10 dataset.\n",
        "2. Part II, Keras Model API: We will use `tf.keras.Model` to define arbitrary neural network architecture. \n",
        "3. Part III, Keras Sequential + Functional API: We will use `tf.keras.Sequential` to define a linear feed-forward network very conveniently, and then explore the functional libraries for building unique and uncommon models that require more flexibility.\n",
        "4. Part IV, CIFAR-10 open-ended challenge: please implement your own network to get as high accuracy as possible on CIFAR-10. You can experiment with any layer, optimizer, hyperparameters or other advanced features. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY3H8DdT_5Ca"
      },
      "source": [
        "# GPU\n",
        "\n",
        "You can manually switch to a GPU device on Colab by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. You should do this before running the following cells to import packages, since the kernel gets restarted upon switching runtimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZOvFii5_5Ca",
        "outputId": "5ddc41b8-f6c7-49c8-caf9-d71c25a112ba",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import timeit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "USE_GPU = True\n",
        "\n",
        "if USE_GPU:\n",
        "    device = '/device:GPU:0'\n",
        "else:\n",
        "    device = '/cpu:0'\n",
        "\n",
        "# Constant to control how often we print when training models.\n",
        "print_every = 100\n",
        "print('Using device: ', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGzEDDSB_5Cb"
      },
      "source": [
        "# Part I: Preparation\n",
        "\n",
        "First, we load the CIFAR-10 dataset. This is a dataset of 32 x 32 RGB images split into 10 classes. The objective is to predict the image label (from 0-9) from the image pixels, a 3D array of shape 32 x 32 x 3. \n",
        "\n",
        "The `tf.keras.datasets` package in TensorFlow provides prebuilt utility functions for loading many common datasets. This might take a few minutes to download the first time you run it, but after that the files should be cached on disk and loading should be faster.\n",
        "\n",
        "For the purposes of this assignment we will still write our own code to preprocess the data and iterate through it in minibatches. The `tf.data` package in TensorFlow provides tools for automating this process, but working with this package adds extra complication and is beyond the scope of this notebook. However using `tf.data` can be much more efficient than the simple approach used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RnE-0CO_5Cb",
        "outputId": "765f1d4f-d451-49ab-94ad-57234026f064",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [],
      "source": [
        "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
        "    \"\"\"\n",
        "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
        "    it for the two-layer neural net classifier. These are the same steps as\n",
        "    we used for the SVM, but condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
        "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10\n",
        "    X_train = np.asarray(X_train, dtype=np.float32)\n",
        "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
        "    X_test = np.asarray(X_test, dtype=np.float32)\n",
        "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = range(num_training, num_training + num_validation)\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = range(num_training)\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = range(num_test)\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean pixel and divide by std\n",
        "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
        "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
        "    X_train = (X_train - mean_pixel) / std_pixel\n",
        "    X_val = (X_val - mean_pixel) / std_pixel\n",
        "    X_test = (X_test - mean_pixel) / std_pixel\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "# If there are errors with SSL downloading involving self-signed certificates,\n",
        "# it may be that your Python version was recently installed on the current machine.\n",
        "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
        "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
        "#   ...replacing paths as necessary.\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "NHW = (0, 1, 2)\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pi-caumOYbIt",
        "outputId": "e702f530-25e0-46bf-94fb-85939caa5dbb"
      },
      "outputs": [],
      "source": [
        "# Visualise a subset of images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(action='ignore', category=Warning, module='matplotlib')\n",
        "labelNames=[\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "\n",
        "def plot_cifar_images(images,class_label):\n",
        "    fig,axes=plt.subplots(3,3,figsize=(20, 15))\n",
        "    fig.subplots_adjust(hspace=0.8,wspace=0.3)\n",
        "\n",
        "    for i,ax in enumerate(axes.flat):\n",
        "        ax.imshow(X_train[i])\n",
        "        xlab=labelNames[int(class_label[i])]\n",
        "        ax.set_xlabel(xlab,fontsize=16)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.xaxis.label.set_color('red')\n",
        "\n",
        "        \n",
        "#Visualize the training images       \n",
        "images=X_train[0:100]\n",
        "cls=y_train[0:100]\n",
        "\n",
        "# Visualize the subset of 10 labels of ytrain\n",
        "\n",
        "print(y_train[0:10])\n",
        "print(np.max(X_train[0]), np.min(X_train[0]))\n",
        "plot_cifar_images(images,cls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvJRRQ5N_5Cc",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [],
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, X, y, batch_size, shuffle=False):\n",
        "        \"\"\"\n",
        "        Construct a Dataset object to iterate over data X and labels y\n",
        "        \n",
        "        Inputs:\n",
        "        - X: Numpy array of data, of any shape\n",
        "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
        "        - batch_size: Integer giving number of elements per minibatch\n",
        "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
        "        \"\"\"\n",
        "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
        "        self.X, self.y = X, y\n",
        "        self.batch_size, self.shuffle = batch_size, shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        N, B = self.X.shape[0], self.batch_size\n",
        "        idxs = np.arange(N)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(idxs)\n",
        "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
        "\n",
        "\n",
        "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
        "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
        "test_dset = Dataset(X_test, y_test, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcpgQdim_5Cd",
        "outputId": "cdc58328-6227-4d09-8b8d-13c145e7fc10"
      },
      "outputs": [],
      "source": [
        "# We can iterate through a dataset like this:\n",
        "for t, (x, y) in enumerate(train_dset):\n",
        "    print(t, x.shape, y.shape)\n",
        "    if t > 5: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPXPVgJA_5Ck",
        "tags": [
          "pdf-ignore"
        ]
      },
      "source": [
        "# Part II: Keras Model Subclassing API\n",
        "\n",
        "TensorFlow 2.0 provides higher-level APIs such as `tf.keras` which make it easy to build models out of modular, object-oriented layers. Further, TensorFlow 2.0 uses eager execution that evaluates operations immediately, without explicitly constructing any computational graphs. This makes it easy to write and debug models, and reduces the boilerplate code.\n",
        "\n",
        "In this part of the notebook we will define neural network models using the `tf.keras.Model` API. To implement your own model, you need to do the following:\n",
        "\n",
        "1. Define a new class which subclasses `tf.keras.Model`. Give your class an intuitive name that describes it, like `TwoLayerFC` or `ThreeLayerConvNet`.\n",
        "2. In the initializer `__init__()` for your new class, define all the layers you need as class attributes. The `tf.keras.layers` package provides many common neural-network layers, like `tf.keras.layers.Dense` for fully-connected layers and `tf.keras.layers.Conv2D` for convolutional layers. **Warning**: Don't forget to call `super(YourModelName, self).__init__()` as the first line in your initializer!\n",
        "3. Implement the `call()` method for your class; this implements the forward pass of your model, and defines the *connectivity* of your network. Layers defined in `__init__()` implement `__call__()` so they can be used as function objects that transform input Tensors into output Tensors. Don't define any new layers in `call()`; any layers you want to use in the forward pass should be defined in `__init__()`.\n",
        "\n",
        "After you define your `tf.keras.Model` subclass, you can instantiate it and use it like the model functions from Part II.\n",
        "\n",
        "### Keras Model Subclassing API: Two-Layer Network\n",
        "\n",
        "Here is a concrete example of using the `tf.keras.Model` API to define a two-layer network. There are a few new bits of API to be aware of here:\n",
        "\n",
        "We use an `Initializer` object to set up the initial values of the learnable parameters of the layers; in particular `tf.initializers.VarianceScaling`. You can read more about it here: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/initializers/VarianceScaling\n",
        "\n",
        "We construct `tf.keras.layers.Dense` objects to represent the two fully-connected layers of the model. In addition to multiplying their input by a weight matrix and adding a bias vector, these layer can also apply a nonlinearity for you. For the first layer we specify a ReLU activation function by passing `activation='relu'` to the constructor; the second layer uses softmax activation function. Finally, we use `tf.keras.layers.Flatten` to flatten the output from the previous fully-connected layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz39_wtF_5Ck",
        "outputId": "3763d924-29fe-4776-b169-782299222e5f",
        "tags": [
          "pdf-ignore-input"
        ]
      },
      "outputs": [],
      "source": [
        "class TwoLayerFC(tf.keras.Model):\n",
        "    def __init__(self, hidden_size, num_classes):\n",
        "        super(TwoLayerFC, self).__init__()        \n",
        "        initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
        "        self.fc1 = tf.keras.layers.Dense(hidden_size, activation='relu',\n",
        "                                   kernel_initializer=initializer)\n",
        "        self.fc2 = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
        "                                   kernel_initializer=initializer)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "    \n",
        "    def call(self, x, training=False):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def test_TwoLayerFC():\n",
        "    \"\"\" A small unit test to exercise the TwoLayerFC model above. \"\"\"\n",
        "    input_size, hidden_size, num_classes = 50, 42, 10\n",
        "    x = tf.zeros((64, input_size))\n",
        "    model = TwoLayerFC(hidden_size, num_classes)\n",
        "    with tf.device(device):\n",
        "        scores = model(x)\n",
        "        print(scores.shape)\n",
        "        \n",
        "test_TwoLayerFC()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QR4VyZb_5Ck"
      },
      "source": [
        "### Exercise 1: Keras Model Subclassing API: Three-Layer ConvNet\n",
        "Now it's your turn to implement a three-layer ConvNet using the `tf.keras.Model` API. Your model should have the following architecture:\n",
        "\n",
        "1. Convolutional layer with 5 x 5 kernels, with zero-padding of 2\n",
        "2. ReLU nonlinearity\n",
        "3. Convolutional layer with 3 x 3 kernels, with zero-padding of 1\n",
        "4. ReLU nonlinearity\n",
        "5. Fully-connected layer to give class scores\n",
        "6. Softmax nonlinearity\n",
        "\n",
        "The number of channels (filters) in the convolutional layers will be provided as arguments in the function. \n",
        "\n",
        "You should initialize the weights of your network using the same initialization method as was used in the two-layer network above.\n",
        "\n",
        "**Hint**: Refer to the documentation for `tf.keras.layers.Conv2D` and `tf.keras.layers.Dense`:\n",
        "\n",
        "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Conv2D\n",
        "\n",
        "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNUwdSG__5Cl"
      },
      "outputs": [],
      "source": [
        "class ThreeLayerConvNet(tf.keras.Model):\n",
        "    def __init__(self, channel_1, channel_2, num_classes):\n",
        "        super(ThreeLayerConvNet, self).__init__()\n",
        "        ########################################################################\n",
        "        # TODO: Implement the __init__ method for a three-layer ConvNet. You   #\n",
        "        # should instantiate layer objects to be used in the forward pass.     #\n",
        "        ########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################\n",
        "        \n",
        "    def call(self, x, training=False):\n",
        "        scores = None\n",
        "        ########################################################################\n",
        "        # TODO: Implement the forward pass for a three-layer ConvNet. You      #\n",
        "        # should use the layer objects defined in the __init__ method.         #\n",
        "        ########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ########################################################################\n",
        "        #                           END OF YOUR CODE                           #\n",
        "        ########################################################################        \n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u23eVVMe_5Cl"
      },
      "source": [
        "Once you complete the implementation of the `ThreeLayerConvNet` above you can run the following to ensure that your implementation does not crash and produces outputs of the expected shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keras_model_output_shape",
        "outputId": "1cf2e48a-769b-48d8-bcee-23457955fd7d"
      },
      "outputs": [],
      "source": [
        "def test_ThreeLayerConvNet():    \n",
        "    channel_1, channel_2, num_classes = 12, 8, 10\n",
        "    model = ThreeLayerConvNet(channel_1, channel_2, num_classes)\n",
        "    with tf.device(device):\n",
        "        x = tf.zeros((64, 3, 32, 32))\n",
        "        scores = model(x)\n",
        "        print(scores.shape)\n",
        "\n",
        "test_ThreeLayerConvNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YptP5K7Q_5Cl"
      },
      "source": [
        "### Keras Model Subclassing API: Eager Training\n",
        "\n",
        "While keras models have a builtin training loop (using the `model.fit`), sometimes you need more customization. Here's an example, of a training loop implemented with eager execution.\n",
        "\n",
        "In particular, notice `tf.GradientTape`. Automatic differentiation is used in the backend for implementing backpropagation in frameworks like TensorFlow. During eager execution, `tf.GradientTape` is used to trace operations for computing gradients later. A particular `tf.GradientTape` can only compute one gradient; subsequent calls to tape will throw a runtime error. \n",
        "\n",
        "TensorFlow 2.0 ships with easy-to-use built-in metrics under `tf.keras.metrics` module. Each metric is an object, and we can use `update_state()` to add observations and `reset_state()` to clear all observations. We can get the current result of a metric by calling `result()` on the metric object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEDjibhR_5Cl",
        "tags": [
          "pdf-ignore"
        ]
      },
      "outputs": [],
      "source": [
        "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
        "    \"\"\"\n",
        "    Simple training loop for use with models defined using tf.keras. It trains\n",
        "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
        "    accuracy on the CIFAR-10 validation set.\n",
        "    \n",
        "    Inputs:\n",
        "    - model_init_fn: A function that takes no parameters; when called it\n",
        "      constructs the model we want to train: model = model_init_fn()\n",
        "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
        "      constructs the Optimizer object we will use to optimize the model:\n",
        "      optimizer = optimizer_init_fn()\n",
        "    - num_epochs: The number of epochs to train for\n",
        "    \n",
        "    Returns: Nothing, but prints progress during trainingn\n",
        "    \"\"\"    \n",
        "    with tf.device(device):\n",
        "\n",
        "        # Compute the loss like we did in Part II\n",
        "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        \n",
        "        model = model_init_fn()\n",
        "        optimizer = optimizer_init_fn()\n",
        "        \n",
        "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "    \n",
        "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
        "        \n",
        "        t = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            \n",
        "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
        "            train_loss.reset_states()\n",
        "            train_accuracy.reset_states()\n",
        "            \n",
        "            for x_np, y_np in train_dset:\n",
        "                with tf.GradientTape() as tape:\n",
        "                    \n",
        "                    # Use the model function to build the forward pass.\n",
        "                    scores = model(x_np, training=is_training)\n",
        "                    loss = loss_fn(y_np, scores)\n",
        "      \n",
        "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "                    \n",
        "                    # Update the metrics\n",
        "                    train_loss.update_state(loss)\n",
        "                    train_accuracy.update_state(y_np, scores)\n",
        "                    \n",
        "                    if t % print_every == 0:\n",
        "                        val_loss.reset_states()\n",
        "                        val_accuracy.reset_states()\n",
        "                        for test_x, test_y in val_dset:\n",
        "                            # During validation at end of epoch, training set to False\n",
        "                            prediction = model(test_x, training=False)\n",
        "                            t_loss = loss_fn(test_y, prediction)\n",
        "\n",
        "                            val_loss.update_state(t_loss)\n",
        "                            val_accuracy.update_state(test_y, prediction)\n",
        "                        \n",
        "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
        "                        print (template.format(t, epoch+1,\n",
        "                                             train_loss.result(),\n",
        "                                             train_accuracy.result()*100,\n",
        "                                             val_loss.result(),\n",
        "                                             val_accuracy.result()*100))\n",
        "                    t += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoD8yhW3_5Cm"
      },
      "source": [
        "### Keras Model Subclassing API: Train a Two-Layer Network\n",
        "We can now use the tools defined above to train a two-layer network on CIFAR-10. We define the `model_init_fn` and `optimizer_init_fn` that construct the model and optimizer respectively when called. Here we want to train the model using stochastic gradient descent with no momentum, so we construct a `tf.keras.optimizers.SGD` function; you can [read about it here](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD).\n",
        "\n",
        "You don't need to tune any hyperparameters here, but you should achieve validation accuracies above 40% after one epoch of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spNwD88P_5Cm",
        "outputId": "c00c4097-71e8-4a6e-ad88-467f767f7056"
      },
      "outputs": [],
      "source": [
        "hidden_size, num_classes = 4000, 10\n",
        "learning_rate = 1e-2\n",
        "\n",
        "def model_init_fn():\n",
        "    return TwoLayerFC(hidden_size, num_classes)\n",
        "\n",
        "def optimizer_init_fn():\n",
        "    return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "train_part34(model_init_fn, optimizer_init_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJH9WPD3_5Cm"
      },
      "source": [
        "### Exercise 2: Keras Model Subclassing  API: Train a Three-Layer ConvNet\n",
        "Here you should use the tools we've defined above to train a three-layer ConvNet on CIFAR-10. Your ConvNet should use 32 filters in the first convolutional layer and 16 filters in the second layer.\n",
        "\n",
        "To train the model you should use gradient descent with momentum 0.9.  \n",
        "\n",
        "**HINT**: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/optimizers/SGD\n",
        "\n",
        "You don't need to perform any hyperparameter tuning, but you should achieve validation accuracies above 50% after training for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keras_model_accuracy",
        "outputId": "4e018205-7961-4f71-8909-2cae18ba73a3"
      },
      "outputs": [],
      "source": [
        "learning_rate = 3e-3\n",
        "channel_1, channel_2, num_classes = 32, 16, 10\n",
        "\n",
        "def model_init_fn():\n",
        "    model = None\n",
        "    ############################################################################\n",
        "    # TODO: Complete the implementation of model_fn.                           #\n",
        "    ############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ############################################################################\n",
        "    #                           END OF YOUR CODE                               #\n",
        "    ############################################################################\n",
        "    return model\n",
        "\n",
        "def optimizer_init_fn():\n",
        "    optimizer = None\n",
        "    ############################################################################\n",
        "    # TODO: Complete the implementation of model_fn.                           #\n",
        "    ############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ############################################################################\n",
        "    #                           END OF YOUR CODE                               #\n",
        "    ############################################################################\n",
        "    return optimizer\n",
        "\n",
        "train_part34(model_init_fn, optimizer_init_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7kTfKK-_5Cn"
      },
      "source": [
        "# Part III: Keras Sequential API\n",
        "In Part II we introduced the `tf.keras.Model` API, which allows you to define models with any number of learnable layers and with arbitrary connectivity between layers.\n",
        "However for many models you don't need such flexibility - a lot of models can be expressed as a sequential stack of layers, with the output of each layer fed to the next layer as input. If your model fits this pattern, then there is an even easier way to define your model: using `tf.keras.Sequential`. You don't need to write any custom classes; you simply call the `tf.keras.Sequential` constructor with a list containing a sequence of layer objects.\n",
        "\n",
        "One complication with `tf.keras.Sequential` is that you must define the shape of the input to the model by passing a value to the `input_shape` of the first layer in your model.\n",
        "\n",
        "### Keras Sequential API: Two-Layer Network\n",
        "In this subsection, we will rewrite the two-layer fully-connected network using `tf.keras.Sequential`, and train it using the training loop defined above.\n",
        "\n",
        "You don't need to perform any hyperparameter tuning here, but you should see validation accuracies above 40% after training for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT1I7MjT_5Cn",
        "outputId": "80f0fd9e-0b11-41d8-e8d3-3b0991ca9e21"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-2\n",
        "\n",
        "def model_init_fn():\n",
        "    input_shape = (32, 32, 3)\n",
        "    hidden_layer_size, num_classes = 4000, 10\n",
        "    initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
        "    layers = [\n",
        "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
        "        tf.keras.layers.Dense(hidden_layer_size, activation='relu',\n",
        "                              kernel_initializer=initializer),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax', \n",
        "                              kernel_initializer=initializer),\n",
        "    ]\n",
        "    model = tf.keras.Sequential(layers)\n",
        "    return model\n",
        "\n",
        "def optimizer_init_fn():\n",
        "    return tf.keras.optimizers.SGD(learning_rate=learning_rate) \n",
        "\n",
        "train_part34(model_init_fn, optimizer_init_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phc9QUNT_5Cn"
      },
      "source": [
        "### Abstracting Away the Training Loop\n",
        "In the previous examples, we used a customised training loop to train models (e.g. `train_part34`). Writing your own training loop is only required if you need more flexibility and control during training your model. Alternately, you can also use  built-in APIs like `tf.keras.Model.fit()` and `tf.keras.Model.evaluate` to train and evaluate a model. Also remember to configure your model for training by calling `tf.keras.Model.compile.\n",
        "\n",
        "You don't need to perform any hyperparameter tuning here, but you should see validation and test accuracies above 42% after training for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtO3bNP8_5Cn",
        "outputId": "058c0caa-a5b6-47d8-fe20-12ba2edba78b"
      },
      "outputs": [],
      "source": [
        "model = model_init_fn()\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbNQ1Hln_5Co"
      },
      "source": [
        "### Exercise 3: Keras Sequential API: Three-Layer ConvNet\n",
        "Here you should use `tf.keras.Sequential` to reimplement the same three-layer ConvNet architecture used in Part II and Part III. As a reminder, your model should have the following architecture:\n",
        "\n",
        "1. Convolutional layer with 32 5x5 kernels, using zero padding of 2\n",
        "2. ReLU nonlinearity\n",
        "3. Convolutional layer with 16 3x3 kernels, using zero padding of 1\n",
        "4. ReLU nonlinearity\n",
        "5. Fully-connected layer giving class scores\n",
        "6. Softmax nonlinearity\n",
        "\n",
        "You should initialize the weights of the model using a `tf.initializers.VarianceScaling` as above.\n",
        "\n",
        "You should train the model using SGD with momentum 0.9.\n",
        "\n",
        "You don't need to perform any hyperparameter search, but you should achieve accuracy above 45% after training for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keras_sequential_accuracy",
        "outputId": "9d95cca8-a048-40cc-dfd7-ea3260a971a4"
      },
      "outputs": [],
      "source": [
        "def model_init_fn():\n",
        "    model = None\n",
        "    ############################################################################\n",
        "    # TODO: Construct a three-layer ConvNet using tf.keras.Sequential.         #\n",
        "    ############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ############################################################################\n",
        "    #                            END OF YOUR CODE                              #\n",
        "    ############################################################################\n",
        "    return model\n",
        "\n",
        "learning_rate = 5e-4\n",
        "def optimizer_init_fn():\n",
        "    optimizer = None\n",
        "    ############################################################################\n",
        "    # TODO: Complete the implementation of model_fn.                           #\n",
        "    ############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate, momentum=0.9)\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    ############################################################################\n",
        "    #                           END OF YOUR CODE                               #\n",
        "    ############################################################################\n",
        "    return optimizer\n",
        "\n",
        "train_part34(model_init_fn, optimizer_init_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsMuur3x_5Co"
      },
      "source": [
        "We will also train this model with the built-in training loop APIs provided by TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41bKcNwN_5Cp",
        "outputId": "877c129f-e0fb-4b17-90fe-20ef7d60849d"
      },
      "outputs": [],
      "source": [
        "model = model_init_fn()\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=1, validation_data=(X_val, y_val))\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdgiuWYT_5Cq"
      },
      "source": [
        "# Part IV: CIFAR-10 open-ended challenge\n",
        "\n",
        "In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n",
        "\n",
        "You should experiment with architectures, hyperparameters, loss functions, regularization, or anything else you can think of to train a model that achieves **at least 70%** accuracy on the **validation** set within 10 epochs. You can use the built-in train function, the `train_part34` function from above, or implement your own training loop.\n",
        "\n",
        "Describe what you did at the end of the notebook.\n",
        "\n",
        "### Some things you can try:\n",
        "- **Filter size**: Above we used 5x5 and 3x3; is this optimal?\n",
        "- **Number of filters**: Above we used 16 and 32 filters. Would more or fewer do better?\n",
        "- **Pooling**: We didn't use any pooling above. Would this improve the model?\n",
        "- **Normalization**: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n",
        "- **Network architecture**: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better?\n",
        "- **Global average pooling**: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n",
        "- **Regularization**: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n",
        "\n",
        "### NOTE: Batch Normalization / Dropout\n",
        "If you are using Batch Normalization and Dropout, remember to pass `is_training=True` if you use the `train_part34()` function. BatchNorm and Dropout layers have different behaviors at training and inference time. `training` is a specific keyword argument reserved for this purpose in any `tf.keras.Model`'s `call()` function. Read more about this here : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods\n",
        "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods\n",
        "\n",
        "### Tips for training\n",
        "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind: \n",
        "\n",
        "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
        "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
        "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
        "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
        "\n",
        "### Going above and beyond\n",
        "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
        "\n",
        "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
        "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
        "- Model ensembles\n",
        "- Data augmentation\n",
        "- New Architectures\n",
        "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
        "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
        "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
        "  \n",
        "### Have fun and happy training! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "open_ended_accuracy"
      },
      "outputs": [],
      "source": [
        "class CustomConvNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(CustomConvNet, self).__init__()\n",
        "        ############################################################################\n",
        "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
        "        ############################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        pass\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ############################################################################\n",
        "        #                            END OF YOUR CODE                              #\n",
        "        ############################################################################\n",
        "    \n",
        "    def call(self, input_tensor, training=False):\n",
        "        ############################################################################\n",
        "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
        "        ############################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        pass\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        ############################################################################\n",
        "        #                            END OF YOUR CODE                              #\n",
        "        ############################################################################\n",
        "        return x\n",
        "\n",
        "\n",
        "print_every = 700\n",
        "num_epochs = 10\n",
        "\n",
        "model = CustomConvNet()\n",
        "\n",
        "def model_init_fn():\n",
        "    return CustomConvNet()\n",
        "\n",
        "def optimizer_init_fn():\n",
        "    learning_rate = 1e-3\n",
        "    return tf.keras.optimizers.Adam(learning_rate) \n",
        "\n",
        "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBfAbZtA_5Cq",
        "tags": [
          "pdf-inline"
        ]
      },
      "source": [
        "## Describe what you did \n",
        "\n",
        "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4YIwvDq_5Cr",
        "tags": [
          "pdf-inline"
        ]
      },
      "source": [
        "**Answer:**\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
