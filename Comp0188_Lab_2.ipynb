{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# COMP0188 - Lab 2\n",
        "\n",
        "Welcome to your second lab session! This week, we will be covering material presented in Lecture 2, conducted on 14 Oct.\n",
        "\n",
        "Learning objectives:\n",
        "1. Gain hands-on experience with implementing neural nets in Tensorflow\n",
        "2. Understand different loss functions and their uses\n",
        "3. Understand overfitting in terms of bias-variance tradeoff\n",
        "4. Understand regularization in terms of bias-variance tradeoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 0 - Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.1 - Setup\n",
        "\n",
        "First, we will ensure all Python libraries are installed and imported"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pip install numpy pandas tensorflow matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLPnzRbr4HUj",
        "outputId": "0d61a2c4-05ce-4c77-ecab-06445e507bdc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "print(\"Imported modules.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will configure Pandas display options. These will come in useful down the line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0.2 - Data Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start off, we will be exploring the [California Housing Dataset](https://developers.google.com/machine-learning/crash-course/california-housing-data-description), a historical dataset of various houses, their attributes, and property sale prices. We will use this to train a simple machine learning model for predicting **house prices**. \n",
        "\n",
        "Because these are public datasets, they can be downloaded directly from a HTTPS URL, allowing us to get away without mounting Google Drive. Hooray! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6PcsGvG6D8h",
        "outputId": "042c0844-3655-4822-9890-3ddfe16c6fdb"
      },
      "outputs": [],
      "source": [
        "# Download the train and test dataset\n",
        "!wget https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\n",
        "!wget https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1 - Neural Nets in Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 - Loading the Data\n",
        "\n",
        "First, let's load the data. \n",
        "\n",
        "The datasets come in the form of tables with several rows and columns. Each row is one unique property and each column describes one particular attribute of the property.\n",
        "\n",
        "We can load the data using Pandas' `read_csv` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijA116qZ6EKf"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(r\"california_housing_train.csv\", delimiter=\",\")\n",
        "test_df = pd.read_csv(\"california_housing_test.csv\", delimiter=\",\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's visualize the data! \n",
        "\n",
        "Printing the dataframe will print all of its rows and columns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "U3wZNdnR6EMO",
        "outputId": "6351deea-81a7-45a0-a3ce-79c860ea8b85"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Too much information! To avoid overflow, we can print the (num_rows, num_cols) as well as just the first k=5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(train_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, let's visualize the test dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "eJT39Skm6EOs",
        "outputId": "15901efe-8cb8-464d-b79b-8007044d155e"
      },
      "outputs": [],
      "source": [
        "print(test_df.shape)\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaKi7jxDXsgM"
      },
      "source": [
        "# Data Processing: Normalize values\n",
        "When building a model with multiple features, the values of each feature should cover roughly the same range. The following code cell normalizes datasets by converting each raw value to its Z-score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8HC-TDgB1D1",
        "outputId": "ffa439fc-44e0-4834-8fa0-743b29ee2a8e"
      },
      "outputs": [],
      "source": [
        "#@title Convert raw values to their Z-scores \n",
        "\n",
        "# Calculate the Z-scores of each column in the training set:\n",
        "train_df_mean = train_df.mean()\n",
        "train_df_std = train_df.std()\n",
        "train_df_norm = (train_df - train_df_mean)/train_df_std\n",
        "\n",
        "# Calculate the Z-scores of each column in the test set.\n",
        "test_df_mean = test_df.mean()\n",
        "test_df_std = test_df.std()\n",
        "test_df_norm = (test_df - test_df_mean)/test_df_std\n",
        "\n",
        "print(\"Normalized the values.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XFqDQmQZnub"
      },
      "source": [
        "# Preparing Dataset for Training\n",
        "\n",
        "The following code cell creates a feature layer containing three features:\n",
        "\n",
        "latitude X longitude (a feature cross)\n",
        "\n",
        "\n",
        "\n",
        "median_income\n",
        "population\n",
        "This code cell specifies the features that you'll ultimately train the model on and how each of those features will be represented. The transformations (collected in my_feature_layer) don't actually get applied until you pass a DataFrame to it, which will happen when we train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vLpVnfjWYOY"
      },
      "outputs": [],
      "source": [
        "# Create an empty list that will eventually hold all created feature columns.\n",
        "feature_columns = []\n",
        "\n",
        "# We scaled all the columns, including latitude and longitude, into their\n",
        "# Z scores. So, instead of picking a resolution in degrees, we're going\n",
        "# to use resolution_in_Zs.  A resolution_in_Zs of 1 corresponds to \n",
        "# a full standard deviation. \n",
        "resolution_in_Zs = 0.3  # 3/10 of a standard deviation.\n",
        "\n",
        "# Create a bucket feature column for latitude.\n",
        "latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n",
        "latitude_boundaries = list(np.arange(int(min(train_df_norm['latitude'])), \n",
        "                                     int(max(train_df_norm['latitude'])), \n",
        "                                     resolution_in_Zs))\n",
        "latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)\n",
        "\n",
        "# Create a bucket feature column for longitude.\n",
        "longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n",
        "longitude_boundaries = list(np.arange(int(min(train_df_norm['longitude'])), \n",
        "                                      int(max(train_df_norm['longitude'])), \n",
        "                                      resolution_in_Zs))\n",
        "longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, \n",
        "                                                longitude_boundaries)\n",
        "\n",
        "# Create a feature cross of latitude and longitude.\n",
        "latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)\n",
        "crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\n",
        "feature_columns.append(crossed_feature)  \n",
        "\n",
        "# Represent median_income as a floating-point value.\n",
        "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
        "feature_columns.append(median_income)\n",
        "\n",
        "# Represent population as a floating-point value.\n",
        "population = tf.feature_column.numeric_column(\"population\")\n",
        "feature_columns.append(population)\n",
        "\n",
        "# Convert the list of feature columns into a layer that will later be fed into\n",
        "# the model. \n",
        "my_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VqSBrBhoiQ_"
      },
      "source": [
        "# Build a linear regression model as a baseline\n",
        "Before creating a deep neural net, find a baseline loss by running a simple linear regression model that uses the feature layer you just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF0BFRXTOeR3",
        "outputId": "eba0f601-07d3-4389-9e9b-d8b737b3d217"
      },
      "outputs": [],
      "source": [
        "#@title Define the plotting function.\n",
        "\n",
        "def plot_the_loss_curve(epochs, mse):\n",
        "  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Mean Squared Error\")\n",
        "\n",
        "  plt.plot(epochs, mse, label=\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
        "  plt.show()  \n",
        "\n",
        "print(\"Defined the plot_the_loss_curve function.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evAr182MWyfO"
      },
      "outputs": [],
      "source": [
        "#@title Define functions to create and train a linear regression model\n",
        "def create_model(my_learning_rate, feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(feature_layer)\n",
        "\n",
        "  # Add one linear layer to the model to yield a simple linear regressor.\n",
        "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n",
        "\n",
        "  # Construct the layers into a model that TensorFlow can execute.\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "  return model           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjQXqu3XpXtL",
        "outputId": "36e541ed-d33c-4b2c-ad87-0295ead32ac0"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataset, epochs, batch_size, label_name):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True)\n",
        "\n",
        "  # Get details that will be useful for plotting the loss curve.\n",
        "  epochs = history.epoch\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  rmse = hist[\"mean_squared_error\"]\n",
        "\n",
        "  return epochs, rmse   \n",
        "\n",
        "print(\"Defined the create_model and train_model functions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68teY0YcpSy1"
      },
      "source": [
        "Run the following code cell to invoke the functions defined in the preceding  two code cells. (Ignore the warning messages.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "Cr0fJohLW6AM",
        "outputId": "0cea38d4-60b6-4814-f494-58194a9b4bce"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 15\n",
        "batch_size = 1000\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "epochs, mse = train_model(my_model, train_df_norm, epochs, batch_size, label_name)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the linear regression model against the test set:\")\n",
        "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5kYPHJWW_vI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhR_inb4qGjS"
      },
      "source": [
        "# Exercise: Train a deep neural network \n",
        "\n",
        "*   Build a newural network with 2 hidden layers\n",
        "*   Experiment with the same learning rate as the linear regression model earlier and compare the results\n",
        "*   Experiment with a different learning rates with a deep net and compare the results\n",
        "*   Bonus: Experiment with different number of perceptrons in each layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPeDvV5xp-9W"
      },
      "source": [
        "# HIDDEN FROM STUDENTS\n",
        "\n",
        "## Define a deep neural net model\n",
        "\n",
        "The `create_model` function defines the topography of the deep neural net, specifying the following:\n",
        "\n",
        "* The number of [layers](https://developers.google.com/machine-learning/glossary/#layer) in the deep neural net.\n",
        "* The number of [nodes](https://developers.google.com/machine-learning/glossary/#node) in each layer.\n",
        "\n",
        "The `create_model` function also defines the [activation function](https://developers.google.com/machine-learning/glossary/#activation_function) of each layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "pedD5GhlDC-y"
      },
      "outputs": [],
      "source": [
        "def create_model(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model by calling the tf.keras.layers.Dense\n",
        "  # method once for each layer. We've specified the following arguments:\n",
        "  #   * units specifies the number of nodes in this layer.\n",
        "  #   * activation specifies the activation function (Rectified Linear Unit).\n",
        "  #   * name is just a string that can be useful when debugging.\n",
        "\n",
        "  # Define the first hidden layer with 20 nodes.   \n",
        "  model.add(tf.keras.layers.Dense(units=20, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  # Define the second hidden layer with 12 nodes. \n",
        "  model.add(tf.keras.layers.Dense(units=12, \n",
        "                                  activation='relu', \n",
        "                                  name='Hidden2'))\n",
        "  \n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  name='Output'))                              \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoZElRASXPS6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jv_lJYTcrEF"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataset, epochs, label_name,\n",
        "                batch_size=None):\n",
        "  \"\"\"Train the model by feeding it data.\"\"\"\n",
        "\n",
        "  # Split the dataset into features and label.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name))\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=True) \n",
        "\n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "  \n",
        "  # To track the progression of training, gather a snapshot\n",
        "  # of the model's mean squared error at each epoch. \n",
        "  hist = pd.DataFrame(history.history)\n",
        "  mse = hist[\"mean_squared_error\"]\n",
        "\n",
        "  return epochs, mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HDlh_ofgXgKZ",
        "outputId": "59451a98-ad3e-4e96-bb80-5a090e688c2a"
      },
      "outputs": [],
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "batch_size = 1000\n",
        "\n",
        "# Specify the label\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set. We're passing the entire\n",
        "# normalized training set, but the model will only use the features\n",
        "# defined by the feature_layer.\n",
        "epochs, mse = train_model(my_model, train_df_norm, epochs, \n",
        "                          label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "# After building a model against the training set, test that model\n",
        "# against the test set.\n",
        "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHoGGc2ChGys"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming that the linear model converged and\n",
        "# the deep neural net model also converged, please \n",
        "# compare the test set loss for each.\n",
        "# In our experiments, the loss of the deep neural \n",
        "# network model was consistently lower than \n",
        "# that of the linear regression model, which \n",
        "# suggests that the deep neural network model \n",
        "# will make better predictions than the \n",
        "# linear regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUfsvNT5vOPb"
      },
      "source": [
        "# You may noticed the following trends:\n",
        "* Two layers outperformed one layer, but \n",
        "three layers did not perform significantly \n",
        "     better than two layers; two layers \n",
        "     outperformed one layer.\n",
        "     In other words, two layers seemed best. \n",
        "* Setting the topography as follows produced \n",
        "     reasonably good results with relatively few \n",
        "     nodes:\n",
        "    * 10 nodes in the first layer.\n",
        "    *  6 nodes in the second layer.\n",
        "\n",
        "As the number of nodes in each layer dropped\n",
        "below the preceding, test loss increased.  \n",
        "However, depending on your application, hardware\n",
        "constraints, and the relative pain inflicted \n",
        "by a less accurate model, a smaller network \n",
        "(for example, 6 nodes in the first layer and \n",
        "4 nodes in the second layer) might be \n",
        "acceptable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-0XFcxjw8_r"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu7R_ZpDopIj"
      },
      "source": [
        "# Addressing the Overfitting Problem  \n",
        "\n",
        "## Regularize the deep neural network (if you have enough time)\n",
        "\n",
        "Notice that the model's loss against the test set is *much higher* than the loss against the training set.  In other words, the deep neural network is [overfitting](https://developers.google.com/machine-learning/glossary/#overfitting) to the data in the training set.  To reduce overfitting, regularize the model.  There several different ways to regularize a model, including:\n",
        "\n",
        "  * [L1 regularization](https://developers.google.com/machine-learning/glossary/#L1_regularization)\n",
        "  * [L2 regularization](https://developers.google.com/machine-learning/glossary/#L2_regularization)\n",
        "  * [Dropout regularization](https://developers.google.com/machine-learning/glossary/#dropout_regularization)\n",
        "\n",
        "Your task is to experiment with one or more regularization mechanisms to bring the test loss closer to the training loss (while still keeping test loss relatively low).  \n",
        "\n",
        "**Note:** When you add a regularization function to a model, you might need to tweak other hyperparameters. \n",
        "\n",
        "### Implementing L1 or L2 regularization\n",
        "\n",
        "To use L1 or L2 regularization on a hidden layer, specify the `kernel_regularizer` argument to [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense). Assign one of the following methods to this argument:\n",
        "\n",
        "* `tf.keras.regularizers.l1` for L1 regularization\n",
        "* `tf.keras.regularizers.l2` for L2 regularization\n",
        "\n",
        "Each of the preceding methods takes an `l` parameter, which adjusts the [regularization rate](https://developers.google.com/machine-learning/glossary/#regularization_rate). Assign a decimal value between 0 and 1.0 to `l`; the higher the decimal, the greater the regularization. For example, the following applies L2 regularization at a strength of 0.01. \n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense(units=20, \n",
        "                                activation='relu',\n",
        "                                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n",
        "                                name='Hidden1'))\n",
        "```\n",
        "\n",
        "### Implementing Dropout regularization\n",
        "\n",
        "You implement dropout regularization as a separate layer in the topography. You implement dropout regularization as a separate layer in the topography. For example, the following code demonstrates how to add a dropout regularization layer between the first hidden layer and the second hidden layer:\n",
        "\n",
        "```\n",
        "model.add(tf.keras.layers.Dense( *define first hidden layer*)\n",
        " \n",
        "model.add(tf.keras.layers.Dropout(rate=0.25))\n",
        "\n",
        "model.add(tf.keras.layers.Dense( *define second hidden layer*)\n",
        "```\n",
        "\n",
        "The `rate` parameter to [tf.keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) specifies the fraction of nodes that the model should drop out during training. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doUSY7v-xZbA"
      },
      "source": [
        "# Exervise: Implement Regularization on your Deep Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AwStMMAIhXdt",
        "outputId": "6b6ef5ed-a47b-4b7d-c21a-4925262e7822"
      },
      "outputs": [],
      "source": [
        "## HIDDERN FROM STUDENTS\n",
        "\n",
        "# The following \"solution\" uses L2 regularization to bring training loss\n",
        "# and test loss closer to each other. Many, many other solutions are possible.\n",
        "\n",
        "\n",
        "def create_model(my_learning_rate, my_feature_layer):\n",
        "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
        "\n",
        "  # Discard any pre-existing version of the model.\n",
        "  model = None\n",
        "\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the layer containing the feature columns to the model.\n",
        "  model.add(my_feature_layer)\n",
        "\n",
        "  # Describe the topography of the model. \n",
        "\n",
        "  # Implement L2 regularization in the first hidden layer.\n",
        "  model.add(tf.keras.layers.Dense(units=20, \n",
        "                                  activation='relu',\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  # Implement L2 regularization in the second hidden layer.\n",
        "  model.add(tf.keras.layers.Dense(units=12, \n",
        "                                  activation='relu', \n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                  name='Hidden2'))\n",
        "\n",
        "  # Define the output layer.\n",
        "  model.add(tf.keras.layers.Dense(units=1,  \n",
        "                                  name='Output'))                              \n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
        "                loss=\"mean_squared_error\",\n",
        "                metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "  return model     \n",
        "\n",
        "# Call the new create_model function and the other (unchanged) functions.\n",
        "\n",
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.007\n",
        "epochs = 140\n",
        "batch_size = 1000\n",
        "\n",
        "label_name = \"median_house_value\"\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, my_feature_layer)\n",
        "\n",
        "# Train the model on the normalized training set.\n",
        "epochs, mse = train_model(my_model, train_df_norm, epochs, \n",
        "                          label_name, batch_size)\n",
        "plot_the_loss_curve(epochs, mse)\n",
        "\n",
        "test_features = {name:np.array(value) for name, value in test_df_norm.items()}\n",
        "test_label = np.array(test_features.pop(label_name)) # isolate the label\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLvIf3r8htgM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ad57050c77180dc9ed5ccc7774a474d285089782a3b5193155c6c81d567ba30"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
