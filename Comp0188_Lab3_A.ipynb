{"cells":[{"cell_type":"markdown","metadata":{"id":"k7UUKrAgo2As"},"source":["# Comp0188 Lab 3\n","\n","Welcome to Lab 3A! \n","\n","In this lab we will introduce you to the Recurrent Neural Network (RNN) model by training it on the MNIST dataset. After visualizing the training history, you will experiment with setting the learning rate using a **learning rate scheduler**. "]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n","\n","As usual, let's begin by installing the necessary libraries. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install numpy\n","!pip install tensorflow\n","!pip install scikit_learn\n","!pip install pydot\n","!pip install graphviz"]},{"cell_type":"markdown","metadata":{},"source":["\n","\n","The basic components of machine learning are:\n","\n","1. The **data**. \n","2. The **loss function**, which serves as the optimization objective. \n","3. The model **parameters**, which will be updated during the training process. \n","4. The **optimizer**, which minimizes the loss function, e.g. stochastic gradient descent or RMSProp."]},{"cell_type":"markdown","metadata":{},"source":["### Data\n","\n","In this lab, we will consider the [MNIST](https://keras.io/api/datasets/mnist/) dataset. It is a dataset consisting of a training set of 60,000 images of handwritten digits, along with a label identifying them from 0-9. We will aim to predict the digit from the image. This problem can be cast as a **classification** problem over 10 distinct classes (the 10 digits).\n","\n","To start off, let's visualize the data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.datasets import mnist\n","\n","\n","# load mnist dataset\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# 1 - Print the number of distinct labels\n","num_labels = len(np.unique(y_train))\n","print(f\"Number of distinct labels in the train set: {num_labels}\")\n","\n","# 2 - Print the first 9 labels\n","print(f\"First 9 labels: {y_train[:9]}\")\n","\n","# 3 - Visualize the first 9 images\n","for i in range(9):\n","\t# define subplot\n","\tplt.subplot(330 + 1 + i)\n","\t# plot raw pixel data\n","\tplt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n","# show the figure\n","print(\"First 9 images: \")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","\n","# convert to one-hot vector\n","print(\"Labels before converting to one-hot: \", y_train[:3])\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","print(\"Labels after converting to one-hot: \")\n","print(y_train[:3])"]},{"cell_type":"markdown","metadata":{},"source":["We'll also **normalize** the raw pixel values. Originally, these are in the range [0,255], but we'll divide by 255 to convert to the range [0,1]\n","\n","Note: be careful to only run the cell below **once**, or you will end up performing the division multiple times!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255"]},{"cell_type":"markdown","metadata":{},"source":["Now, we'll define our model architecture. The model will accept input of size $(B, H, W)$ where $H = W = 28$ is our image size and $B$ is the batch size. It will output labels of size $(B, K)$ where $K=10$ is our number of classes.The outputs of the model are interpreted as a **score** for each class - the class with the highest score is taken to be the model's prediction. \n","\n","We will construct the model as an RNN with a dense classification layer on top. At a high level, the RNN processes each image sequentially - in this case, it processes the image one row (28 pixels) at a time, storing past information in its **hidden state**, until it reaches the end, at which point it has seen the full image. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1555,"status":"ok","timestamp":1666728471649,"user":{"displayName":"Priya Lakshmi","userId":"12105358925717388855"},"user_tz":-60},"id":"FlpUmVXro0IK","outputId":"725df277-3ed0-4818-ef48-cb087366606e"},"outputs":[],"source":["import numpy as np\n","from tensorflow.python.keras.models import Sequential\n","from tensorflow.python.keras.layers import Dense, Activation, SimpleRNN\n","from tensorflow.keras.utils import plot_model\n","\n","# network parameters\n","input_shape = (28, 28)\n","batch_size = 128\n","units = 256\n","dropout = 0.2\n","\n","# model is RNN with 256 units, input is 28-dim vector 28 timesteps\n","model = Sequential()\n","model.add(SimpleRNN(units=units,\n","                    activation='tanh',\n","                    dropout=dropout,\n","                    input_shape=input_shape))\n","model.add(Dense(num_labels))\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Loss Function:                                                     \n","A loss function provides a smooth optimization objective that can be used to improve the model. Put simply, a lower loss function means a better model, and differentiating the loss function gives a gradient over the model parameters which can be used by standard optimizers \n","\n","For the task of classification, we commonly use **cross-entropy**,  which quantifies how well our predicted class labels agree with our ground-truth labels. The higher level of agreement between these two sets of labels, the lower our loss (and higher our classification accuracy, at least on the training set). \n","\n","As the model outputs unnormalized scores or **logits**, we specify `from_logits=True` in the loss function constructor. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":641227,"status":"ok","timestamp":1666732503029,"user":{"displayName":"Priya Lakshmi","userId":"12105358925717388855"},"user_tz":-60},"id":"140-LTYGpc8R","outputId":"5c420f93-51ef-41ee-c92b-4286584ec48a"},"outputs":[],"source":["from tensorflow.keras.losses import CategoricalCrossentropy\n","\n","model.compile(loss=CategoricalCrossentropy(from_logits=True),\n","              optimizer='RMSprop',\n","              metrics=['accuracy'])\n","# train the network\n","model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n","\n","_, acc = model.evaluate(x_test,\n","                        y_test,\n","                        batch_size=batch_size,\n","                        verbose=0)\n","\n","print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))\n","\n","### RMSProp test acc is 97.4 ###\n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise 1 - Learning Rates\n","\n","The default learning rate used by RMSProp is $0.001$. Now re-train the model with different learning rates of $0.01$ and $0.0001$ respectively. What do you notice? Which learning rate performs best? \n","\n","Hints:\n","1. Look up the documentation for `RMSProp` optimizer in Tensorflow to see how to set the learning rate. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Your code here ###"]},{"cell_type":"markdown","metadata":{"id":"5p35JR2oZu90"},"source":["## Exercise 2 - Learning Rate Schedulers:\n","\n","The concept of learning rate schedule is sometimes called learning rate **annealing** or **adaptive learning rates**. Intuitively, the model at the start does not know much, so a higher learning rate helps it ramp up its performance quickly. Near the end, when the model is close to an optimal solution, a lower learning rate prevents it from leaving the optimal region and 'forgetting' what it already knew. By adjusting our learning rate on an epoch-to-epoch basis, we can reduce loss, increase accuracy, and even in certain situations reduce the total amount of time it takes to train a network.\n","\n","Train the RNN model with RMSprop with a learning rate decay factor of `0.25`, and report the final test accuracy and validation accuracy. How does this compare with your results in Exercise 1?\n","\n","Hint: \n","1. Look up the `LearningRateScheduler` callback function in Tensorflow. \n","2. Look at the documentation of `model.fit` in Tensorflow to see how to introduce the learning rate callback in the training loop.\n","3. You may find the following function, which defines a learning rate at each epoch, to be useful in configuring the scheduler. As a bonus, to understand how the `step_decay` function works, you can write some code (using `matplotlib.pyplot`) to visualize the learning rate provided at each epoch. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def step_decay(epoch):\n","    initial_alpha = 0.2\n","    factor = 0.25\n","    decayE = 5\n","\n","    #\n","    alpha = initial_alpha*(factor**np.ceil((1+epoch)/decayE))\n","    print(epoch, alpha)\n","    return np.float(alpha)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### Your code here ###"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNLE5/DDC2PWJGNdH42bppC","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"0ad57050c77180dc9ed5ccc7774a474d285089782a3b5193155c6c81d567ba30"}}},"nbformat":4,"nbformat_minor":0}
